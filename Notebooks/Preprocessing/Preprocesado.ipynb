{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf04db33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# Configuracion global\n",
    "CHUNK_TIME = 30 # En segundos\n",
    "SAMPLE_RATE = 250 # En Hz\n",
    "CHUNK_SIZE = math.floor(CHUNK_TIME * SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fad782f",
   "metadata": {},
   "source": [
    "# Leer tabla RAW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc42e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer la RAW\n",
    "directory = \"../Raws/Segunda_Temporada_Grabaciones/\"\n",
    "\n",
    "recording_file = directory + \"RAW_COMPLETA_17-04-2024.csv\"\n",
    "RAW_DATA = pd.read_csv(recording_file)\n",
    "METADATA_COLUMNS = RAW_DATA.columns[0:9]\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "display(RAW_DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf3c2e2",
   "metadata": {},
   "source": [
    "# Preprocesar BrainAccess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb27997a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "from sklearn.decomposition import FastICA\n",
    "\n",
    "#Canales del casco\n",
    "EEG_CHANNELS = 16\n",
    "\n",
    "#Columnas correspondientes a EEG\n",
    "EEG_COLUMNS = RAW_DATA.columns[9:28]\n",
    "#Columnas EEG sin acelerometro\n",
    "EEG_COLUMNS_FOR_ICA = RAW_DATA.columns[9:25]\n",
    "\n",
    "RAW_EEG = RAW_DATA[METADATA_COLUMNS].join(RAW_DATA[EEG_COLUMNS])\n",
    "preprocessed_eeg = RAW_EEG.dropna(subset=EEG_COLUMNS)\n",
    "\n",
    "# Definir un filtro, por ejemplo, un filtro paso bajo Butterworth\n",
    "ORDER = 6\n",
    "FS = 2000  # Frecuencia de muestreo (Hz)\n",
    "FC = 300  # Frecuencia de corte (Hz)\n",
    "b, a = signal.butter(ORDER, FC / (FS / 2), 'low')\n",
    "\n",
    "# Configuracion de ICA\n",
    "ica = FastICA(n_components=EEG_CHANNELS, random_state=42)\n",
    "\n",
    "# Definir la cantidad de últimas medidas para calcular la media\n",
    "N = int(CHUNK_TIME * SAMPLE_RATE)\n",
    "\n",
    "# Separar la RAW por takes\n",
    "eeg_endtimes = {}\n",
    "eeg_takes = {}\n",
    "\n",
    "for (subjectID, take), eeg_take in preprocessed_eeg.groupby(['Subject ID', 'Take']):\n",
    "    take_name = f\"Subject {subjectID} Take {take}\"\n",
    "\n",
    "    any_nan_rows = eeg_take.isnull().any(axis=1).sum()\n",
    "    assert any_nan_rows == 0, \"No se han eliminado todos los NaN\"\n",
    "\n",
    "    #Guardamos el tiempo final para escalar EmotiBit mas adelante\n",
    "    eeg_endtimes[take_name] = eeg_take[\"time\"].iloc[-1]\n",
    "\n",
    "    expectedTimeEntries = int(eeg_endtimes[take_name] * SAMPLE_RATE + 1)\n",
    "    assert eeg_take.shape[0] == expectedTimeEntries, \"Se esperan \" + str(expectedTimeEntries) + \" entradas de tiempo del EEG. Hay \" + str(eeg_take.shape[0])\n",
    "\n",
    "    # Aplicar el filtro a cada columna por separado\n",
    "    for column in EEG_COLUMNS_FOR_ICA:\n",
    "        eeg_take[column] = signal.filtfilt(b, a, eeg_take[column])\n",
    "\n",
    "    # Aplicar FastICA a las columnas del EEG\n",
    "    eeg_take[EEG_COLUMNS_FOR_ICA] = ica.fit_transform(eeg_take[EEG_COLUMNS_FOR_ICA])\n",
    "\n",
    "    # Crear la nueva columna \"media activación\" para cada electrodo\n",
    "    for electrode in range(EEG_CHANNELS):  \n",
    "        col_name = f'{EEG_COLUMNS_FOR_ICA[electrode]}_media_activacion'\n",
    "        eeg_take[col_name] = eeg_take.iloc[:, electrode + 8].rolling(window=N).mean()\n",
    "\n",
    "    eeg_takes[take_name] = eeg_take\n",
    "\n",
    "# Reconstruir tabla completa\n",
    "preprocessed_eeg = pd.concat(eeg_takes.values(), ignore_index=True)\n",
    "\n",
    "display(preprocessed_eeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07f3c6b",
   "metadata": {},
   "source": [
    "# Preprocesar EmotiBit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87801bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Columnas correspondientes a EmotiBit\n",
    "# Dropear tags no biometricas\n",
    "EMOTIBIT_COLUMNS = RAW_DATA.columns[30:51].drop([\"ER\"])\n",
    "\n",
    "RAW_EMOTIBIT = RAW_DATA[METADATA_COLUMNS].join(RAW_DATA[EMOTIBIT_COLUMNS])\n",
    "preprocessed_emotibit = RAW_EMOTIBIT.dropna(subset=EMOTIBIT_COLUMNS, how='all')\n",
    "\n",
    "# Configuracion del StardardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Separar la RAW por takes\n",
    "emotibit_takes = {}\n",
    "valid_start_times = {}\n",
    "\n",
    "for (subjectID, take), emotibit_take in preprocessed_emotibit.groupby(['Subject ID', 'Take']):\n",
    "    take_name = f\"Subject {subjectID} Take {take}\"\n",
    "\n",
    "    # Escalar el tiempo\n",
    "    emotibit_endtime = emotibit_take[\"time\"].iloc[-1]\n",
    "    emotibit_take[\"time\"] = emotibit_take[\"time\"] * eeg_endtimes[take_name] / emotibit_endtime\n",
    "\n",
    "    # Añadir las entradas de tiempo regular\n",
    "    emotibit_take = emotibit_take.merge(eeg_takes[take_name][[\"time\"]], on=\"time\", how=\"outer\")\n",
    "\n",
    "    emotibit_take = emotibit_take.sort_values(by=\"time\")\n",
    "\n",
    "    # Mantener el valor hasta que encontremos uno nuevo\n",
    "    emotibit_take.ffill(inplace=True)\n",
    "\n",
    "    # Eliminar entradas de tiempo irregular\n",
    "    emotibit_take = emotibit_take[emotibit_take[\"time\"].isin(eeg_takes[take_name][\"time\"])]\n",
    "\n",
    "    # Los valores de las columnas HR y BI tardan varios segundos en llegar, \n",
    "    # por lo que debemos cortar los valores anteriores a que lleguen\n",
    "    valid_start_times[take_name] = CHUNK_TIME\n",
    "\n",
    "    any_nan_rows = emotibit_take[EMOTIBIT_COLUMNS].isnull().any(axis=1).sum()\n",
    "    time_with_nan = any_nan_rows / SAMPLE_RATE\n",
    "    if (time_with_nan > valid_start_times[take_name]):\n",
    "        valid_start_times[take_name] = time_with_nan\n",
    "        print(\"Los datos del sujeto \" + str(take_name) + \" no se completan hasta el segundo \" + str(time_with_nan))\n",
    "\n",
    "    # Aplicar StandardScaler a las columnas seleccionadas\n",
    "    emotibit_take[EMOTIBIT_COLUMNS] = scaler.fit_transform(emotibit_take[EMOTIBIT_COLUMNS])\n",
    "\n",
    "    expectedTimeEntries = int(eeg_endtimes[take_name] * SAMPLE_RATE + 1)\n",
    "    assert emotibit_take.shape[0] == expectedTimeEntries, \"Se esperan \" + str(expectedTimeEntries) + \" entradas de tiempo del EEG. Hay \" + str(emotibit_take.shape[0])\n",
    "\n",
    "    emotibit_takes[take_name] = emotibit_take\n",
    "\n",
    "# Reconstruir tabla completa\n",
    "preprocessed_emotibit = pd.concat(emotibit_takes.values(), ignore_index=True)\n",
    "\n",
    "display(preprocessed_emotibit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9df156d",
   "metadata": {},
   "source": [
    "# TableApp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89450004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columnas correspondientes a TableApp\n",
    "TABLEAPP_COLUMNS = RAW_DATA.columns[69:]\n",
    "\n",
    "RAW_TABLEAPP = RAW_DATA[METADATA_COLUMNS].join(RAW_DATA[TABLEAPP_COLUMNS])\n",
    "preprocessed_tableapp = RAW_TABLEAPP.dropna(subset=TABLEAPP_COLUMNS)\n",
    "\n",
    "# Reescalar emociones\n",
    "preprocessed_tableapp.loc[:, TABLEAPP_COLUMNS] = (preprocessed_tableapp[TABLEAPP_COLUMNS] - 3) / (5 - 3)\n",
    "\n",
    "# Separar la RAW por takes\n",
    "tableapp_takes = {}\n",
    "\n",
    "for (subjectID, take), tableapp_take in preprocessed_tableapp.groupby(['Subject ID', 'Take']):\n",
    "    take_name = f\"Subject {subjectID} Take {take}\"\n",
    "\n",
    "    # Añadir las entradas de tiempo regular\n",
    "    tableapp_take = tableapp_take.merge(eeg_takes[take_name][[\"time\"]], on=\"time\", how=\"outer\")\n",
    "    \n",
    "    tableapp_take = tableapp_take.sort_values(by=\"time\")\n",
    "    \n",
    "    # Rellenar NaN replicando la ultima entrada hasta que se encuentre una nueva\n",
    "    tableapp_take.ffill(inplace=True)\n",
    "\n",
    "    # Eliminar entradas de tiempo irregular\n",
    "    tableapp_take = tableapp_take[tableapp_take[\"time\"].isin(eeg_takes[take_name][\"time\"])]\n",
    "\n",
    "    any_nan_rows = tableapp_take.isnull().any(axis=1).sum()\n",
    "    assert any_nan_rows == 0, \"No se han eliminado todos los NaN\"\n",
    "\n",
    "    expectedTimeEntries = int(eeg_endtimes[take_name] * SAMPLE_RATE + 1)\n",
    "    assert tableapp_take.shape[0] == expectedTimeEntries, \"Se esperan \" + str(expectedTimeEntries) + \" entradas de tiempo del EEG. Hay \" + str(tableapp_take.shape[0])\n",
    "\n",
    "    tableapp_takes[take_name] = tableapp_take\n",
    "\n",
    "# Concatenar de nuevo todos los takes\n",
    "preprocessed_tableapp = pd.concat(tableapp_takes.values(), ignore_index=True)\n",
    "\n",
    "display(preprocessed_tableapp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb26bb8",
   "metadata": {},
   "source": [
    "# Generar tabla preprocesada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedf906d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "preprocessed_data = pd.concat([preprocessed_eeg, preprocessed_emotibit[EMOTIBIT_COLUMNS], preprocessed_tableapp[TABLEAPP_COLUMNS]], axis=1)\n",
    "\n",
    "preprocessed_takes = {}\n",
    "\n",
    "for (subjectID, take), preprocessed_take in preprocessed_data.groupby(['Subject ID', 'Take']):\n",
    "    take_name = f\"Subject {subjectID} Take {take}\"\n",
    "    \n",
    "    # Recortar el principio y el final\n",
    "    preprocessed_take = preprocessed_take[preprocessed_take['time'] >= valid_start_times[take_name]]\n",
    "    preprocessed_take = preprocessed_take[preprocessed_take['time'] < math.floor(eeg_endtimes[take_name])]\n",
    "\n",
    "    any_nan_rows = tableapp_take.isnull().any(axis=1).sum()\n",
    "    assert any_nan_rows == 0, \"No se han eliminado todos los NaN\"\n",
    "\n",
    "    preprocessed_takes[take_name] = preprocessed_take\n",
    "\n",
    "# Concatenar de nuevo todos los takes\n",
    "preprocessed_data = pd.concat(preprocessed_takes.values(), ignore_index=True)\n",
    "\n",
    "training_input = preprocessed_data.drop(columns=METADATA_COLUMNS).drop(columns=TABLEAPP_COLUMNS)\n",
    "\n",
    "#Reestructuracion en filas con un chunk de informacion\n",
    "expanded_columns = [f'{col}_t{((i - 1) / SAMPLE_RATE):.3f}' for col in training_input.columns for i in range(1, CHUNK_SIZE + 1)]\n",
    "\n",
    "rows, cols = training_input.shape\n",
    "compressed_data = np.zeros((rows // CHUNK_SIZE, cols * CHUNK_SIZE))\n",
    "\n",
    "for row in range(0, rows // CHUNK_SIZE):\n",
    "    start_idx = row * CHUNK_SIZE\n",
    "    end_idx = start_idx + CHUNK_SIZE\n",
    "    \n",
    "    for idx, col in enumerate(training_input.columns):\n",
    "        col_data = training_input[col].values[start_idx:end_idx]\n",
    "        compressed_data[row, idx * CHUNK_SIZE:(idx + 1) * CHUNK_SIZE] = col_data\n",
    "\n",
    "training_input = pd.DataFrame(compressed_data, columns=expanded_columns)\n",
    "\n",
    "display(training_input)\n",
    "\n",
    "any_nan_rows = training_input.isnull().any(axis=1).sum()\n",
    "assert any_nan_rows == 0, \"No se han eliminado todos los NaN\"\n",
    "\n",
    "# Comprime emotions_df tomando el primer valor de cada grupo de filas con la misma frecuencia que la compresión de sorted_preprocessed_data\n",
    "compressed_tableapp = preprocessed_data.iloc[::CHUNK_SIZE]\n",
    "compressed_tableapp = compressed_tableapp[TABLEAPP_COLUMNS]\n",
    "display(compressed_tableapp.iloc[:-1])\n",
    "#Volver a añadir emotions\n",
    "training_input.reset_index(drop=True, inplace=True)\n",
    "compressed_tableapp.reset_index(drop=True, inplace=True)\n",
    "merged_df = pd.concat([training_input, compressed_tableapp.iloc[:-1]], axis=1)\n",
    "#merged_df = pd.merge(training_input, compressed_tableapp, left_index=True, right_index=True)\n",
    "display(merged_df)\n",
    "any_nan_rows = merged_df.isnull().any(axis=1).sum()\n",
    "assert any_nan_rows == 0, \"No se han eliminado todos los NaN\" + str(any_nan_rows)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dc87c7",
   "metadata": {},
   "source": [
    "# Exportar tabla preprocesada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9b347a",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv(\"../Preprocesadas/Preprocessed_Data_Chunk_30_seg_17-04-2024.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
